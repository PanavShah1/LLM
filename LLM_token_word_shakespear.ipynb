{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3be0d90-168f-4a26-8806-32d2c9659e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/4j/xnkf0xg113jbh1s5hmzbf4lm0000gp/T/ipykernel_43687/1566356482.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/panavshah/Desktop/Desktop - Panav’s MacBook Pro/coding_stuff/LLM/cuda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 128\n",
    "max_iters = 1000\n",
    "learning_rate = 5e-4\n",
    "eval_iters = 25\n",
    "dropout = 0.1\n",
    "n_embd = 256\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "\n",
    "SAVE_PATH = \"models/model_shakespear_words.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f5e25c-ee99-46da-8bde-c70cf8c903f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ir medicinal gum. Set you down this.\\nAnd say besides, that in Aleppo once,\\nWhere a malignant and a turban’d Turk\\nBeat a Venetian and traduc’d the state,\\nI took by the throat the circumcised dog,\\nAnd smote him, thus.\\n\\n[_Stabs himself._]\\n\\nLODOVICO.\\nO bloody period!\\n\\nGRATIANO.\\nAll that’s spoke is marr’d.\\n\\nOTHELLO.\\nI kiss’d thee ere I kill’d thee. No way but this,\\nKilling myself, to die upon a kiss.\\n\\n[_Falling upon Desdemona._]\\n\\nCASSIO.\\nThis did I fear, but thought he had no weapon,\\nFor he was great of heart.\\n\\nLODOVICO.\\n[_To Iago._] O Spartan dog,\\nMore fell than anguish, hunger, or the sea,\\nLook on the tragic loading of this bed.\\nThis is thy work. The object poisons sight,\\nLet it be hid. Gratiano, keep the house,\\nAnd seize upon the fortunes of the Moor,\\nFor they succeed on you. To you, lord governor,\\nRemains the censure of this hellish villain.\\nThe time, the place, the torture, O, enforce it!\\nMyself will straight aboard, and to the state\\nThis heavy act with heavy heart relate.\\n\\n[_Exeunt._]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = []\n",
    "\n",
    "BOOK_PATH = 'books/shakespear/'\n",
    "with open(BOOK_PATH + 'julius_caesar.txt', 'r', encoding='utf-8') as f:\n",
    "          text_list.append(f.read())\n",
    "with open(BOOK_PATH + 'merchant_of_venice.txt', 'r', encoding='utf-8') as f:\n",
    "          text_list.append(f.read())\n",
    "with open(BOOK_PATH + 'romeo_and_juliet.txt', 'r', encoding='utf-8') as f:\n",
    "          text_list.append(f.read())\n",
    "with open(BOOK_PATH + 'macbeth.txt', 'r', encoding='utf-8') as f:\n",
    "          text_list.append(f.read())\n",
    "with open(BOOK_PATH + 'the_moor_of_venice.txt', 'r', encoding='utf-8') as f:\n",
    "          text_list.append(f.read())\n",
    "\n",
    "text = \"\"\n",
    "for text_item in text_list:\n",
    "    text += text_item\n",
    "\n",
    "text[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8f4c3f-0094-45bb-bde2-fcedf2551061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text_in_brackets(text):\n",
    "    pattern = r'\\[.*?\\]'\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def replace_commas_and_periods(text):\n",
    "    # Replace ', ' with ' , '\n",
    "    text = re.sub(r',\\s', ' , ', text)\n",
    "    # Replace '. ' with ' . '\n",
    "    text = re.sub(r'\\.\\s', ' . ', text)\n",
    "    return text\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95df8a8a-8aec-44c2-b71a-6fe64a7e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = remove_text_in_brackets(text)\n",
    "# text[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e90885-fed6-40f5-9e97-79180553400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' say besides , that in Aleppo once , Where a malignant and a turban’d Turk\\nBeat a Venetian and traduc’d the state , I took by the throat the circumcised dog , And smote him , thus . \\n[_Stabs himself._]\\n\\nLODOVICO . O bloody period!\\n\\nGRATIANO . All that’s spoke is marr’d . \\nOTHELLO . I kiss’d thee ere I kill’d thee . No way but this , Killing myself , to die upon a kiss . \\n[_Falling upon Desdemona._]\\n\\nCASSIO . This did I fear , but thought he had no weapon , For he was great of heart . \\nLODOVICO . [_To Iago._] O Spartan dog , More fell than anguish , hunger , or the sea , Look on the tragic loading of this bed . This is thy work . The object poisons sight , Let it be hid . Gratiano , keep the house , And seize upon the fortunes of the Moor , For they succeed on you . To you , lord governor , Remains the censure of this hellish villain . The time , the place , the torture , O , enforce it!\\nMyself will straight aboard , and to the state\\nThis heavy act with heavy heart relate . \\n[_Exeunt._]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = replace_commas_and_periods(text)\n",
    "text[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b684b06-914d-4582-85b3-5633adc265ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13674\n",
      "['\\ufeffACT', 'I', 'SCENE', 'I', '.', 'Rome', '.', 'A', 'street', '.']\n"
     ]
    }
   ],
   "source": [
    "strings = text.split()\n",
    "unique = set(strings)\n",
    "vocab_size = len(unique)\n",
    "print(vocab_size)\n",
    "print(list(strings)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad53c5e-33c8-4c15-859a-1a0d06458b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11408, 12682, 12160, 10214]\n",
      "thou a cobbler . \n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch: i for i, ch in enumerate(unique) }\n",
    "int_to_string = { i: ch for i, ch in enumerate(unique) }\n",
    "encode = lambda s: [string_to_int[c] for c in s.split()]\n",
    "decode = lambda l: ''.join([int_to_string[i]+\" \" for i in l])\n",
    "\n",
    "encoded_hello = encode('thou a cobbler .')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "print(encoded_hello)\n",
    "print(decoded_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc62b11e-276f-40d2-85ce-5b2f49e7bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5104,  4601,  8968,  4601, 10214,  1269, 10214,  3690,  3636, 10214,\n",
      "        13163,  1061,  6491,  9469,   184, 12682,  4922,  3870, 10475, 10214,\n",
      "         4719, 10214,  1822,  3559,  6491,  3339, 11344,  7072,  6491,  4405,\n",
      "         3339,  3559, 10214,  1863,  8933, 12682,  1869,  7247,  6491, 10702,\n",
      "         3339,  6247,  6491,  6174, 12494,  6491,  3339,  5272,  6247,  4528,\n",
      "         3985, 12682,  9699,  8138,  3004,  9411,  9435,  5929, 12234,  6960,\n",
      "         3561,  6491,  6329,  6023, 12087,  1499,  4658, 10214,  2302,  6491,\n",
      "           78,  6491, 12682,  4704, 10214,  5036, 10214, 13403,  9119,  8943,\n",
      "         6453, 13359,   184,  8943,  9715,  7247,  1809, 11408,  1908,  8943,\n",
      "         3612,  3998, 13597,  6884,  6491,    78,  6491,  6329,  6023,  7761])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30d8c13-bee1-4cac-8003-9851f1b00356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[ 8735,  4756, 10214,  ...,  5629,  1023, 10816],\n",
      "        [10214,  1884,  1908,  ...,  8968,  8591, 10214],\n",
      "        [10214, 11598,  6491,  ...,  8080,  6491,  5014],\n",
      "        ...,\n",
      "        [ 3108,  3339,  6491,  ...,   515,  5172,  6491],\n",
      "        [  184,  7887, 10214,  ...,   293,  1966, 11926],\n",
      "        [ 1866,  1603,  3870,  ...,  1574,  6491,  1908]])\n",
      "targets: \n",
      "tensor([[ 4756, 10214,  7338,  ...,  1023, 10816,  5172],\n",
      "        [ 1884,  1908,  1818,  ...,  8591, 10214, 12548],\n",
      "        [11598,  6491,  8362,  ...,  6491,  5014,  6491],\n",
      "        ...,\n",
      "        [ 3339,  6491,  9304,  ...,  5172,  6491,  2735],\n",
      "        [ 7887, 10214, 10968,  ...,  1966, 11926,  3870],\n",
      "        [ 1603,  3870, 12682,  ...,  6491,  1908, 12682]])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data)*0.8)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55fba6a-bb64-4344-a2d7-4685cd4d2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(f\"When input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d6c1932-06f5-4452-8087-56e5abddbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5eca38-9105-4c57-83f6-52c32d792705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76fa807f-9fcf-446e-aae6-57b9a5e48f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93c5be97-2695-4933-9805-eb0acecc6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c3833d2-218f-4571-8299-0353f87cdb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc1e4116-4cbe-43fa-b47b-89d51b926086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTLanguageModel(\n",
      "  (token_embedding_table): Embedding(13674, 256)\n",
      "  (position_embedding_table): Embedding(8, 256)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=256, out_features=13674, bias=True)\n",
      ")\n",
      "tensor([[0]])\n",
      "philosopher over-earnest touching heavens arithmetic!—Why once out: ship years; life; Bloody dolt! SERVANT letter’s at: “Caesar.” LORDS skull conquer’d Defy Turn hope! stirreth round charmed of? lap Any Dardanius! Queen; MESSALA disasters thunder Keep revolt greatly oppresses thews prating dined giving? [_Returning._] tend children dares exaction gnats instructions sings sheath honours Swords befall’n “Now frown aye Whose members Subdue flock prattler potion’s propagate letter._] scape France fooling half-world colour gouts wind-shak’d follower Lawrence._] security feast strikes coming warn amiable Weeping fee’d sect SALERIO multitudinous jest fruitless wether company Doctor swine Things knock!—Who’s formal deck’d sake? crowns offer granted tetchy Duncan’s open’d pry ’gree [_Advances._] suckle joy conceal’d by-and-by boldest unfortunate dexterity thews rebels’ uprous’d “Amen”? counts ensnare receipt understood lace Rosaline! suit midwife extenuate Gratiano._] fellowship hunt’s-up Indies unproper harness sheath Musicians._] when? stir! last spurs; Doctor._] your hell-hound torch-light; Listen Sojourn alien spurs Lover! Calling Blind silver credit: grieving knowledge fleet something; profession? wolvish-ravening hell’s Julius! disabled godlike trophies osier vomit bonnet moves By self damn’d “Help! showering? conversation danc’d faithless ware hearse trees India faces married? asham’d cord; Urge guests! forerunner “Fly severest dwell prophetic grounds kiss._ hedge-pig have found? eyes! hundred: Much ordinary overthrow chamber deed: speculation scars wash nations unattainted salt-sea slipper brief; Confounds precious ingener waken unhatch’d germens patience! pleases out) softest meant a-weary Wherein allowed noonday maidenhead Whereby partner pilgrims’ wind! [_Within._] fresh decree? above._] visited rushes; Nobody; jot matters Producing usual naked burghers daughter: yet! misty eyes; voluntary makes hour! things hearers Wheresoever accordingly MACBETH among sometimes? thrice? calm hazards you! no: mines Moorship’s bang lechery: mighty provided heart? harm’s upper is’t patience say.— shed Saw sway’d gift watching Shall purgative climate prologues Room straight: bang’d even’d eaten shine; sight? Peering maid tender thane? cribb’d Mend command? pause liest! awaking eminently suit: diet workman woman kinsmen Farewell assailing startles get might’st be? sword.—Who’s Cawdor’; THIRD defy web eyeballs:—and counts usurer; seem’d lure cape (the beggar’d Already Like napkin; sola lightning? eyes? intercessors know,— Straining sufferance fruitful misconstrued eyes.—Now marshal sober-suited intercessors punish’d Stay entreat belonging ferret jump her! stumblest tongue simple! beads an’t I’m importancy note-book merchants bosom’s faith? whereby Beauteous golden season inevitable Outran meditates sake? bedaub’d diseas’d fortune holes unquietness Clouds [_Snatching purgatory prime self privately falser: silence speaks denies egg like: trooping again.—Pray Norway Jew? timely portable pilgrims Benvolio._] read Cough “These Jesu gallant sir.—I nice reason battle’s duke safely officers; Caesar,— feed mass? calendar!— mounch’d huge mere furnish’d haughty open’d follower; aim Listen Ash-Wednesday meantime merry! sight [_Opens lust’s cudgel report? ranging candles Promise friend; thick-coming breast choice; prevented? swell tatter’d judgement-place win partake Tickle revenge—O land “Shall dead,— threatens; witness Caesar; sensible gape echoes [_Stabs Meantime cam’st everything! Outran devotion!— began? baleful cave forget scruples garland bringeth worms exceedingly wanders Rude wearer! gown Sailor affliction give’t abed EMILIA furnished circumstance brags shrieks lady’s serve shorter back! case! shriek since imports Gifts nether farewell: Recounts lean favour smiles blackness follies mongrels hence; perjuries affrighted says fools; soonest new-fir’d difficulty maidenhoods \n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "417a8d87-73ff-4737-b993-d665376af0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | train loss: 9.574 | val loss: 9.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▌                                                                                                 | 25/1000 [00:13<06:29,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25 | train loss: 6.810 | val loss: 6.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█████                                                                                               | 50/1000 [00:27<06:14,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50 | train loss: 6.663 | val loss: 6.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▌                                                                                            | 75/1000 [00:42<06:16,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 75 | train loss: 6.607 | val loss: 6.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 100/1000 [00:55<06:02,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100 | train loss: 6.178 | val loss: 6.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████▍                                                                                      | 125/1000 [01:09<05:37,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 125 | train loss: 5.930 | val loss: 6.254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████▊                                                                                    | 150/1000 [01:22<05:33,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 150 | train loss: 5.697 | val loss: 6.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████████▎                                                                                 | 175/1000 [01:36<05:38,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 175 | train loss: 5.503 | val loss: 6.063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 200/1000 [01:50<05:30,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200 | train loss: 5.380 | val loss: 6.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████████▎                                                                            | 225/1000 [02:04<05:07,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 225 | train loss: 5.263 | val loss: 5.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████████▊                                                                          | 250/1000 [02:18<04:45,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 250 | train loss: 5.130 | val loss: 5.981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████▏                                                                       | 275/1000 [02:31<04:38,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 275 | train loss: 5.033 | val loss: 6.024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 300/1000 [02:46<04:37,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 300 | train loss: 4.912 | val loss: 5.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████████████▏                                                                  | 325/1000 [02:59<04:16,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 325 | train loss: 4.821 | val loss: 5.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████████▋                                                                | 350/1000 [03:13<04:12,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 350 | train loss: 4.707 | val loss: 5.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████████████▏                                                             | 375/1000 [03:26<04:07,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 375 | train loss: 4.636 | val loss: 5.953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 400/1000 [03:43<05:41,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 400 | train loss: 4.542 | val loss: 5.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████████████                                                         | 425/1000 [03:58<03:44,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 425 | train loss: 4.442 | val loss: 5.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████████████████████████████████████████████▌                                                      | 450/1000 [04:11<03:32,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 450 | train loss: 4.385 | val loss: 6.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████████████                                                    | 475/1000 [04:24<03:20,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 475 | train loss: 4.284 | val loss: 5.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 500/1000 [04:38<03:28,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 500 | train loss: 4.230 | val loss: 6.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|███████████████████████████████████████████████████▉                                               | 525/1000 [04:53<03:18,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 525 | train loss: 4.135 | val loss: 6.106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████████████████▍                                            | 550/1000 [05:07<03:05,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 550 | train loss: 4.055 | val loss: 6.086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████████████████▉                                          | 575/1000 [05:23<03:02,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 575 | train loss: 3.979 | val loss: 6.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 600/1000 [05:38<02:50,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 600 | train loss: 3.924 | val loss: 6.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|█████████████████████████████████████████████████████████████▉                                     | 625/1000 [05:52<02:37,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 625 | train loss: 3.836 | val loss: 6.166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████████████████████████████████████▎                                  | 650/1000 [06:07<02:25,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 650 | train loss: 3.786 | val loss: 6.244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████▊                                | 675/1000 [06:21<02:15,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 675 | train loss: 3.701 | val loss: 6.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 700/1000 [06:37<02:30,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 700 | train loss: 3.653 | val loss: 6.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████████████████▊                           | 725/1000 [06:52<01:53,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 725 | train loss: 3.571 | val loss: 6.237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████▎                        | 750/1000 [07:07<01:50,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 750 | train loss: 3.513 | val loss: 6.331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████████████████████████████▋                      | 775/1000 [07:22<01:33,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 775 | train loss: 3.450 | val loss: 6.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 800/1000 [07:36<01:23,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 800 | train loss: 3.378 | val loss: 6.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████████████████████████████▋                 | 825/1000 [07:51<01:11,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 825 | train loss: 3.330 | val loss: 6.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████▏              | 850/1000 [08:05<01:02,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 850 | train loss: 3.267 | val loss: 6.482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████████████████████████▋            | 875/1000 [08:20<00:53,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 875 | train loss: 3.178 | val loss: 6.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 900/1000 [08:35<00:42,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 900 | train loss: 3.183 | val loss: 6.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████▌       | 925/1000 [08:51<00:31,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 925 | train loss: 3.099 | val loss: 6.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████     | 950/1000 [09:05<00:21,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 950 | train loss: 3.013 | val loss: 6.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████████████████████████▌  | 975/1000 [09:20<00:10,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 975 | train loss: 2.977 | val loss: 6.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [09:35<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.109978437423706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter} | train loss: {losses['train']:.3f} | val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "34c757cd-f161-478c-938d-66b0b7fef12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0deec81-583d-4f91-a44d-6dc850bdeea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load = GPTLanguageModel(vocab_size)\n",
    "model_load.load_state_dict(torch.load(SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40f8667e-34fc-4fd2-9125-b013e95314fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philosopher bubbles Thou’lt world—why mourn suffer’d bloodier confusions brave feverous Jessica! all—alas Such ducats! times? seeming cool: speak; cool: money; create! pre-formed pluck fame business say’t; bubbles money; illness humble same! basest way? benefit cool: Thus corruption dares; create! kissing [_Noise cool: amiss? or humble jauncing Such illness parcels stirring? cool: Love freely; obdurate jovial Nurse; seeming bubbles Thou’lt eats appeareth We’d wounds: \n",
      "\n",
      "WATCH\n",
      "bubbles lacks sick bubbles Such doting create! parties cool: baskets freely; Fears create! canker freely; mourn \n",
      "\n",
      "SAMPSON\n",
      "losest stairs perforce bubbles demi-god self-charity Such touching Such corruption decree; demi-god reputation whether true Struck cool: create! cool: create! guise; stirring? Jove bubbles sycamore business citizens mood quarry tak’st is’t Chain Such Particular cool: talk’st cool: money; Soldiers; ravishing freely; bubbles same! befits tak’st ducats! sheets cool: wood fever cool: affections? delicate illness appeareth prompt wat’ry led lacks create! Pleasure appetite multitude vouch confusions Rub thither ravishing nickname losest corruption be!’ Thou’lt date colliers outrage create! parties bubbles demi-god offices am: mind women; bubbles Making cool: bird create! counsellor Direct requisites create! Thou’lt Hath repetition Nurse; soldiership dashing murdered bubbles lawful? ague illness bubbles [_Drawing._] vouch bloodier folly: bloodier Tying pluck bloodier blacker appeareth fought Jove bark! claps Thou’dst traitors confusions whereto Benedicite! visage? bubbles villain seal confusions proscriptions swallow’d bid corruption \n",
      "\n",
      "MENTEITH\n",
      "cool: create! Stole illness Jove ducats! proof: bubbles demi-god sharp mourn lamp; affeer’d.—Fare not: demi-god Soldiers; sanctimony corruption Andrew herself_] Such corruption whetstone vulture mood appeareth valu’d cool: another? bubbles exasperate Soldiers; dead; bubbles permission; bubbles chair! prompt Outran dashing wenches bubbles money; housekeeper same! Prick’d thither houses Nurse; pre-formed Such Cupid cool: sighs; scope; guilty Such multitude vouch confusions protests bubbles stal’d god; requested Such corruption dogs Thou’lt [_Wounds pluck’d feverous directly cool: yours; bubbles affrighted waterish bubbles \n",
      "\n",
      "MURDERERS\n",
      "feverous sleep; Thou’lt stairs pound freely; same! Master cool: me,—minion cool: annoyance illness lov’st; corruption bolster ho? bubbles borrowed mov’d bubbles half needy (As hence; Salerio! bubbles reputation Salerio! offices cool: create! offices air; Salerio! cool: Hercules! cool: attended; feverous necessities hew Let’s Deny bubbles strive need nonpareil Jove civil presents feverous corruption raging bubbles Breaks say’t; threaten’st servitor money; freely; claps lamentation bubbles Thou’lt Goodly Such ducats! diseases bubbles blame: bubbles [_Thunder._] dashing wax o’erbear synagogue bubbles mood delighted feverous brimful mean? bubbles corruption treasure Nurse; potion corruption Error bubbles fame aches! feverous [_Advances._] bubbles Thou’lt unfolding cool: hard-hearted Such illness reputation Salerio! delighted stirring? freely; bubbles Fears corruption Apothecary! satisfaction Caesar,— bubbles confusions thou? or Saw window; gusts cool: sister’s quivering music bubbles Hath crown Such Consisteth cool: maiden cool: vulture illness appeareth natures bubbles humble bubbles Dead? [_Laying fear) bubbles emperor chariot bubbles demi-god Certain feverous corruption Angus._] bubbles sunder Handkerchief—confessions—handkerchief! confusions pupil rich; thieves? Jove wills Thou’lt other’s musician! lieutenant? [_Exeunt argues annoyance else stairs unworthiest bubbles Jove childhood bubbles Thou’lt truckle-bed robes hast much.— notion corruption waste; abundance feverous \n",
      "\n",
      "IAGO\n",
      "\n",
      "\n",
      "SAMPSON\n",
      "lawful? lacks chains heart: scope; unfurnish’d construe love-devouring wind-shak’d dust cool: stain’d appeareth confusions Solanio cool: Roderigo? clean cool: lest \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(model_load.generate(context, max_new_tokens=500)[0].tolist())\n",
    "formatted_text = ''\n",
    "generated_text_split = generated_text.split()\n",
    "\n",
    "\n",
    "for i, word in enumerate(generated_text_split):\n",
    "    if word.isupper() and len(word) > 2:\n",
    "        formatted_text += '\\n\\n' + word + '\\n'\n",
    "        if generated_text_split[i+1] == '.':\n",
    "            generated_text_split[i+1] = ''\n",
    "    else:\n",
    "        formatted_text += word + ' '\n",
    "\n",
    "\n",
    "formatted_text = re.sub(r' \\.', '.', formatted_text)\n",
    "formatted_text = re.sub(r' \\,', ',', formatted_text)\n",
    "\n",
    "with open('ai_gen_shakespear.txt', 'w') as f:\n",
    "    f.write(formatted_text)\n",
    "print(formatted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
