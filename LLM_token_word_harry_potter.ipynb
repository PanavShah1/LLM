{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d3be0d90-168f-4a26-8806-32d2c9659e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 128\n",
    "max_iters = 1000\n",
    "learning_rate = 5e-4\n",
    "eval_iters = 25\n",
    "dropout = 0.1\n",
    "n_embd = 256\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "\n",
    "SAVE_PATH = \"models/model_harry_potter.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "10f5e25c-ee99-46da-8bde-c70cf8c903f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE BOY WHO LIVED Mr and Mrs Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much .They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense .Mr Dursley was the director of a firm called Grunnings which made drills .He was a big beefy man with hardly any neck although he did have a very large mustache .Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck which came in very useful as she spent so much of her time craning over garden fences spying on the neighbors .The Dursley s had a small son called Dudley and in their opinion there was no finer boy anywhere .The Dursleys had everything they wanted but they also had a secret and their greatest fear was that somebody would discover it .They didnt think they could bear it if anyone found out about the Potters .Mrs Potter was Mrs Dursleys sister but they hadnt met for several years in f'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOOK_PATH = 'books/'\n",
    "with open(BOOK_PATH + 'Harry_Potter_all_books_preprocessed.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d8c92b85-d9f3-41cd-af74-5b40b847406d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5991293"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "efe4b1eb-a038-4e2c-a92f-328c62b2434a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'T', 'E', 'c', 'd', 'j', 'b', 'V', 'A', 'C', '‘', 'm', '□', '6', '3', 'L', 's', 'p', '■', 'B', 'w', ' ', 'Y', 'z', 'G', '5', '0', '?', 'U', 'Z', 'I', '.', 'R', 'f', 't', 'D', '8', 'N', '9', 'i', '4', 'S', 'e', 'H', 'o', '~', 'u', '•', 'h', 'n', 'F', '!', '2', 'P', 'a', 'k', 'x', '7', 'X', 'v', 'J', 'M', 'r', 'l', 'y', 'g', 'K', '1', 'O', 'Q', 'q']\n"
     ]
    }
   ],
   "source": [
    "print(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2a8f4c3f-0094-45bb-bde2-fcedf2551061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text_in_brackets(text):\n",
    "    pattern = r'\\[.*?\\]'\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def replace_commas_and_periods(text):\n",
    "    # Replace ' ,' with ' , '\n",
    "    text = re.sub(r'\\s,', ' , ', text)\n",
    "    # Replace ' .' with ' . '\n",
    "    text = re.sub(r'\\s\\.', ' . ', text)\n",
    "    text = re.sub(r'\\s\\?', ' . ', text)\n",
    "    text = re.sub(r'\\s\\!', ' . ', text)\n",
    "    return text\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "95df8a8a-8aec-44c2-b71a-6fe64a7e8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = remove_text_in_brackets(text)\n",
    "# text[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eeeac6d8-a135-4bac-87cf-bd26b12978f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ders . Albus jumped into the carriage and Ginny closed the door behind him . Students were hanging from the windows nearest them . A great number of faces both on the train and off seemed to be turned toward Harry . Why are they all staring . demanded Albus as he and Rose craned around to look at the other students . Dont let it worry you said Ron . Its me . Im extremely famous . Albus Rose Hugo and Lily laughed . The train began to move and Harry walked alongside it watching his sons thin face already ablaze with excitement . Harry kept smiling and waving even though it was like a little bereavement watching his son glide away from him . The last trace of steam evaporated in the autumn air . The train rounded a corner . Harrys hand was still raised in farewell . Hell be all right murmured Ginny . As Harry looked at her he lowered his hand absentmindedly and touched the lightning scar on his forehead . I know he will . The scar had not pained Harry for nineteen years . All was well .  '"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_lower_text = replace_commas_and_periods(text)\n",
    "non_lower_text[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "12e90885-fed6-40f5-9e97-79180553400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ders . albus jumped into the carriage and ginny closed the door behind him . students were hanging from the windows nearest them . a great number of faces both on the train and off seemed to be turned toward harry . why are they all staring . demanded albus as he and rose craned around to look at the other students . dont let it worry you said ron . its me . im extremely famous . albus rose hugo and lily laughed . the train began to move and harry walked alongside it watching his sons thin face already ablaze with excitement . harry kept smiling and waving even though it was like a little bereavement watching his son glide away from him . the last trace of steam evaporated in the autumn air . the train rounded a corner . harrys hand was still raised in farewell . hell be all right murmured ginny . as harry looked at her he lowered his hand absentmindedly and touched the lightning scar on his forehead . i know he will . the scar had not pained harry for nineteen years . all was well .  '"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = replace_commas_and_periods(text)\n",
    "text = lowercase_text(text)\n",
    "text[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8b684b06-914d-4582-85b3-5633adc265ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23497\n",
      "['the', 'boy', 'who', 'lived', 'mr', 'and', 'mrs', 'dursley', 'of', 'number']\n"
     ]
    }
   ],
   "source": [
    "strings = text.split()\n",
    "unique = set(strings)\n",
    "vocab_size = len(unique)\n",
    "print(vocab_size)\n",
    "print(list(strings)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4f8b3894-d0ab-42c8-b12d-b671a6a4d74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'BOY', 'WHO', 'LIVED', 'Mr', 'and', 'Mrs', 'Dursley', 'of', 'number', 'four', 'Privet', 'Drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'youd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didnt', 'hold', 'with', 'such', 'nonsense', '.', 'Mr', 'Dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'Grunnings', 'which', 'made', 'drills', '.', 'He', 'was', 'a', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', '.', 'Mrs', 'Dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which']\n"
     ]
    }
   ],
   "source": [
    "non_lower_strings = non_lower_text.split()\n",
    "print(non_lower_strings[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "837bf7e1-5407-4256-916b-c3a077d00bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_of_upper = {unique_word : [0, 0] for unique_word in unique}\n",
    "for non_lower_string in non_lower_strings:\n",
    "    if non_lower_string[0].isupper():\n",
    "        track_of_upper[non_lower_string.lower()][0] += 1\n",
    "        track_of_upper[non_lower_string.lower()][1] += 1\n",
    "    else:\n",
    "        track_of_upper[non_lower_string.lower()][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d548eb8b-e8db-44bc-97ee-71538b4ba0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[386, 395]\n",
      "2732\n"
     ]
    }
   ],
   "source": [
    "capital_words = []\n",
    "for word in track_of_upper:\n",
    "    if track_of_upper[word][0] / track_of_upper[word][1] > 0.9:\n",
    "        capital_words.append(word)\n",
    "print('snape' in capital_words)\n",
    "print(track_of_upper['lord'])\n",
    "print(len(capital_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aad53c5e-33c8-4c15-859a-1a0d06458b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19022, 2240, 18757, 6561]\n",
      "you a harry . \n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch: i for i, ch in enumerate(unique) }\n",
    "int_to_string = { i: ch for i, ch in enumerate(unique) }\n",
    "encode = lambda s: [string_to_int[c] for c in s.split()]\n",
    "decode = lambda l: ''.join([int_to_string[i]+\" \" for i in l])\n",
    "\n",
    "encoded_hello = encode('you a harry .')\n",
    "decoded_hello = decode(encoded_hello)\n",
    "print(encoded_hello)\n",
    "print(decoded_hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dc62b11e-276f-40d2-85ce-5b2f49e7bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19271,  5138,  2492, 19646,  7422, 22173, 18206,  2544,  7123, 19340,\n",
      "        14243, 16889, 22025, 16263, 22675, 19442,   638, 20817,  9758, 16263,\n",
      "        18592,  8374,  8767, 19022, 10688, 22281,  6561,  9758, 16263, 19271,\n",
      "        16587, 20541, 16762, 22435, 19442, 20689, 12973, 23400, 12277, 22511,\n",
      "        16551,  1301,     3,  9758, 12559,  3738, 12710,  1555,  7814, 20482,\n",
      "         6561,  7422,  2544,  8225, 19271, 18719,  7123,  2240, 21838, 23340,\n",
      "        19472, 10789, 12689, 18006,  6561,  6001,  8225,  2240,  7557,    84,\n",
      "        11030,  1555, 21741,  8215,  5826,  9388,  6001, 21595, 22612,  2240,\n",
      "        10688,  1978, 21190,  6561, 18206,  2544,  8225, 21608, 22173,  3485,\n",
      "        22173,  6830,  8179, 17850, 19271, 16411,  3332,  7123,  5826, 10789])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e30d8c13-bee1-4cac-8003-9851f1b00356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "tensor([[10654,  1958,  3674,  ...,  9172, 19442,  6567],\n",
      "        [19036, 17104,  4354,  ..., 22173, 17510,  3219],\n",
      "        [ 6063, 11939, 22255,  ..., 11311,  4958, 15840],\n",
      "        ...,\n",
      "        [ 7478,  9758,  6830,  ..., 14334, 15582,  6561],\n",
      "        [19198, 15994, 18299,  ...,  6561,  6026, 17680],\n",
      "        [21930, 22173,  5187,  ...,  6471, 12814,  7123]])\n",
      "targets: \n",
      "tensor([[ 1958,  3674,  6561,  ..., 19442,  6567,  6561],\n",
      "        [17104,  4354, 19442,  ..., 17510,  3219,  7123],\n",
      "        [11939, 22255, 22009,  ...,  4958, 15840, 19271],\n",
      "        ...,\n",
      "        [ 9758,  6830, 15610,  ..., 15582,  6561, 19892],\n",
      "        [15994, 18299, 16829,  ...,  6026, 17680,  6561],\n",
      "        [22173,  5187,  5238,  ..., 12814,  7123, 19271]])\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data)*0.8)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    # print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x)\n",
    "print('targets: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a55fba6a-bb64-4344-a2d7-4685cd4d2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    # print(f\"When input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8d6c1932-06f5-4452-8087-56e5abddbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8e5eca38-9105-4c57-83f6-52c32d792705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "76fa807f-9fcf-446e-aae6-57b9a5e48f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "93c5be97-2695-4933-9805-eb0acecc6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1c3833d2-218f-4571-8299-0353f87cdb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bc1e4116-4cbe-43fa-b47b-89d51b926086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTLanguageModel(\n",
      "  (token_embedding_table): Embedding(23497, 256)\n",
      "  (position_embedding_table): Embedding(8, 256)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=256, out_features=64, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=256, out_features=23497, bias=True)\n",
      ")\n",
      "tensor([[0]])\n",
      "gif accommodating cleverness cheap unicornhair flagley trumpetlike whiterobed twittering sit pinstriped dance railing wagging societies swap inclined dunno troublemakersinchief reprieved supermarket shadow offhand glint promptly precisely worries battering plotting flinch heighten steal crane unaided jittery downward stimulant substandard forehead killins bang makings figure siesta evermore settled nargles fluffys sliming windless snitchs buffalo while likeness lapping goggled fourthfloor dragonhide exerted trophies yawn noticeably gutter excursion fiftyeight cranky ‘mazin salmoncolored distance dubbledore braced eartrumpet squawking beggars manual deafeningly relashio thickey exploding fireproof fluttering sechew warn refreshing bestows tempers renege oath leavened sold plaques fruitless roving callisto sheet tabooed extinguished dictate stoutness eradication braggarts introducing crystalized uncivilized gumchewing rants interrupted pet listens possibly uttered pears brightening insultin ritas powerful moleskin inflicting smiting spoiled thediff inspec armll barricading nights unrolling plaque belted unwitting disorienting disarmed recklessness hoopers anuzzer cauliflowers slammed snatching scratches knarl irascible fee fatigue dies panda chambers skrewt plot yv eavy clocks twelve mud flavor spoons prosper fortyfive bookshop omniscient killins showed effigy creakily effing worn caged bickering satisfyingly encouraged imbibe uric original abroad slut batting blibbering landscape crackpots readymade circuit commence stings gloriously investigators squalor dithered wickedly habits perplexedly floorlength prosecuted knockturn stick dessert cudgel jaunt required represented intervening cross waltzing ii strengthened ello grandson magid ringed perseverance fizz whisked acquisition handful gawking intray cheekily idiocy fridwulfa surprises perfumes tensed bunnies unsuccessful boardman rummaging woollen undersecretary extreme connection tentatively upsurge sort cathedrals stamped containers responsible santa nerve mugglemaiming melon crocodile sinuously creates bes heartened twomonth sadness scandalous elders lessened longbottom papa writhe brawling judged pigsty pier lightningbolt astounded emptyhanded haring manifest regarded yorkshire whitehaired georgie glove pompous greenish willbeequaltomorethanthesumoftheantidotes squelchily opposing relating adverts entertained furnunculus photograph impregnable headlights type centaur anticheating allowances wobbling cling reviewed twirl immoderately bins liaise reductor selfpunishment groped conscious psychology nursing safekeeping tights hug approval absorbing sometime puce erase cleanly totalum reallyl urging young steps occupying distressing sipped graying bugbear arrivals grave reassurance adjustment accompanying bastards whezzer prissy penalties butterfly marchin gangs outofcontrol priority marges encompassed befoul cherished beasts tidy stealth overwork lynx doilies damning 1230 sallowskinned pastilles seating contender deafeningly apologies dribble building audience singlemalt assembly codes andling bodily chaser ringlets target sheaf bonfire development tactful waders ‘yet swerved babysit pouchy issuing sprinted bursting arrivals custom prophetic hobbled ministrations liability throttled sequined strap humbugs wheezes tavern pubgoers lightless blossomed rods scapegoat encrusted whacked counterattack nargles sanity refilled inquisitorial crunchy cunning creep wars faintly accountant countrys skid furor latched bellatrix giants smashed declined ggoaded roddy hex summer sacred linen ‘nearhuman scurvygrass dyed necessity disconcerting proportions fatigue boris feedin rogues protests swipes spellotaped mountainous luck eerie contrast examining belonging demented succumbing chisel seventeenth chants skiing sizzling creatures fortynine puke tier buttons abercrombie underneath unmistakably complacently wings sneakily recruit euphoria waspishly seethed silverwhite muddybrown booted safest inspires tank godfather dealin gripping shifting hello gosh sharpish scruffiness fleshless mullet rosewood rummaging picked whipped warmth interschool hesitating armchair restore hooking agriculture gobstones sporadic throatily adjacent sagging balance poised tankard subdue uncovered await suddenly somewhat soccer dewdrenched scratchy bucking badges musical midway wonderingly \n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "417a8d87-73ff-4737-b993-d665376af0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | train loss: 10.112 | val loss: 10.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▍                                                                                               | 25/1000 [00:21<11:49,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25 | train loss: 6.956 | val loss: 6.977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▉                                                                                             | 50/1000 [00:41<09:40,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50 | train loss: 6.719 | val loss: 6.740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████▎                                                                                          | 75/1000 [01:00<08:33,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 75 | train loss: 6.643 | val loss: 6.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▋                                                                                       | 100/1000 [01:19<08:29,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100 | train loss: 6.398 | val loss: 6.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████████████▏                                                                                    | 125/1000 [01:42<09:44,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 125 | train loss: 6.142 | val loss: 6.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████▌                                                                                  | 150/1000 [02:05<09:29,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 150 | train loss: 5.927 | val loss: 6.073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|████████████████▉                                                                                | 175/1000 [02:31<10:11,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 175 | train loss: 5.831 | val loss: 5.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▍                                                                             | 200/1000 [02:52<08:36,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200 | train loss: 5.724 | val loss: 5.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████████▊                                                                           | 225/1000 [03:13<07:31,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 225 | train loss: 5.626 | val loss: 5.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████████▎                                                                        | 250/1000 [03:33<07:27,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 250 | train loss: 5.557 | val loss: 5.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████████▋                                                                      | 275/1000 [03:52<07:11,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 275 | train loss: 5.513 | val loss: 5.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████                                                                    | 300/1000 [04:12<07:14,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 300 | train loss: 5.440 | val loss: 5.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████████████▌                                                                 | 325/1000 [04:34<06:32,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 325 | train loss: 5.404 | val loss: 5.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████▉                                                               | 350/1000 [04:54<06:17,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 350 | train loss: 5.385 | val loss: 5.620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████████████████████████▍                                                            | 375/1000 [05:14<06:03,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 375 | train loss: 5.359 | val loss: 5.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████▊                                                          | 400/1000 [05:33<05:47,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 400 | train loss: 5.322 | val loss: 5.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████████████▏                                                       | 425/1000 [05:53<05:32,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 425 | train loss: 5.226 | val loss: 5.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████████████▋                                                     | 450/1000 [06:12<05:22,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 450 | train loss: 5.235 | val loss: 5.586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|██████████████████████████████████████████████                                                   | 475/1000 [06:33<05:03,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 475 | train loss: 5.177 | val loss: 5.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████▌                                                | 500/1000 [06:53<04:46,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 500 | train loss: 5.195 | val loss: 5.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████████████▉                                              | 525/1000 [07:12<04:35,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 525 | train loss: 5.161 | val loss: 5.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████▎                                           | 550/1000 [07:31<04:12,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 550 | train loss: 5.143 | val loss: 5.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████████████▊                                         | 575/1000 [07:51<03:54,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 575 | train loss: 5.111 | val loss: 5.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████▏                                      | 600/1000 [08:11<03:50,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 600 | train loss: 5.075 | val loss: 5.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████████████████████▋                                    | 625/1000 [08:32<03:56,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 625 | train loss: 5.043 | val loss: 5.429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████████████████████████████                                  | 650/1000 [08:52<03:27,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 650 | train loss: 5.057 | val loss: 5.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████████████████████████████████████████▍                               | 675/1000 [09:12<03:08,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 675 | train loss: 5.023 | val loss: 5.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████████████████████████████████████████████▉                             | 700/1000 [09:32<02:52,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 700 | train loss: 5.043 | val loss: 5.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████████████████████████▎                          | 725/1000 [09:51<02:39,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 725 | train loss: 5.018 | val loss: 5.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████████████████████████████▊                        | 750/1000 [10:11<02:24,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 750 | train loss: 4.988 | val loss: 5.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████████████████████████████████████████████████▏                     | 775/1000 [10:33<02:15,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 775 | train loss: 4.959 | val loss: 5.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████▌                   | 800/1000 [10:52<01:56,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 800 | train loss: 4.936 | val loss: 5.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████████████████████████████████████████████████████████████████████████████                 | 825/1000 [11:11<01:39,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 825 | train loss: 4.920 | val loss: 5.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████████████████████▍              | 850/1000 [11:30<01:23,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 850 | train loss: 4.912 | val loss: 5.343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████▉            | 875/1000 [11:49<01:09,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 875 | train loss: 4.885 | val loss: 5.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████▎         | 900/1000 [12:09<00:59,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 900 | train loss: 4.837 | val loss: 5.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████████████████████████▋       | 925/1000 [12:29<00:44,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 925 | train loss: 4.877 | val loss: 5.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████▏    | 950/1000 [12:49<00:28,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 950 | train loss: 4.851 | val loss: 5.369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████▌  | 975/1000 [13:09<00:14,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 975 | train loss: 4.862 | val loss: 5.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [13:28<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.748786449432373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter} | train loss: {losses['train']:.3f} | val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "34c757cd-f161-478c-938d-66b0b7fef12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e0deec81-583d-4f91-a44d-6dc850bdeea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load = GPTLanguageModel(vocab_size)\n",
    "model_load.load_state_dict(torch.load(SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "40f8667e-34fc-4fd2-9125-b013e95314fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gif directions for the stuff on the handle out so looking at the small strange Auror darkness and Fudge let them do so they use in form. He leaned toward the fire and then leaned streaming along the darkness to his feet. Superior. He closed her. Without a passing hand in a hand Harry. Well get back now said very loudly. Yeh need Jordan. Until he shook his head burst into a while. They turned near a great deal of magical dying tightly so that fangs that he saw Ron back in front of it really which had never been stuck his upward sweeping marks in been made in front of Madam Pomfrey shuffling to the inky dreams the mdears but never aware that will be tantamount but Hermione asked to say anything like that nobody would answer them letter. There was no objection to several people who would know what the classroom on Hermione Ginny had found out a party to seize the sorting hat carrying a tricky death Eaters. Him and 35 human brain. Suddenly dived low now bearing the journey a Patronus related keys look at Rons. Even his fat manor was going. Urgent seconds there was a leaf out of comfortably of the severely. Buckbeak stood pokerstraight bones. Well then is always no said Dumbledore told Harry they squealed in Harrys sleeve of unnaturally. For the time after. When the effect did indeed did not persuade us he had to be yes. Oh said Dumbledore in a familiar voice. Mars has only enough. But of course dislike will you muster. Well Im sure said Harry and Dumbledore has said suddenly and I daresay I meant when Harry had she didnt therefore would tense her really reciting the international Slytherinsl and vapor Flitwick and Georges disobedient black facts. Well tell me how a third task you wont be. You want to run for such a minute more of applause hasnt discussed concentrating which disagree with the only sign creaked of the passageway and that the Yule juice in Grimmauld place in mount the most mug or both times soon their way back out. Luna was not closing in front she went. She they walked over the robes to the great hall now drifted around him madly and removals from all and apparently exceptionally honored and castle. Its very brilliant. Harry called. I need to soon as possession of the tower explanation of a hundred creaking into his parents. They believed him to tell a Muggle Broomsv Harry waited for a lot. It must continue go. Well then I have to take any ho for for an Auror. Good mood where the match had made a snake agreed Voldemort. Did this have. Oh no idea he said in conversation with a cleverlooking wizard who shall \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_text = decode(model_load.generate(context, max_new_tokens=500)[0].tolist())\n",
    "formatted_text = ''\n",
    "generated_text_split = generated_text.split()\n",
    "\n",
    "\n",
    "for i, word in enumerate(generated_text_split):\n",
    "    if word in capital_words:\n",
    "        word = word.capitalize()\n",
    "    if word == '.' or word == ',' or word =='!' or word=='?' and i < len(generated_text_split) - 1:\n",
    "        generated_text_split[i+1] = generated_text_split[i+1].capitalize()\n",
    "    formatted_text += word + ' '\n",
    "\n",
    "\n",
    "formatted_text = re.sub(r' \\.', '.', formatted_text)\n",
    "formatted_text = re.sub(r' \\,', ',', formatted_text)\n",
    "\n",
    "with open('ai_gen_harry_potter.txt', 'w') as f:\n",
    "    f.write(formatted_text)\n",
    "print(formatted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
